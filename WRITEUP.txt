What is the theoretical worst-case running time of the algorithm you implemented
(i.e. in Î˜-notation), expressed in terms of the number of words n in the input
file? Justify your answer.

For each string in words.txt, the value is normalized and a frequency hash is generated. This results
in the worst-case time complexity being O(nm) time, with m being the length of the longest word and n
being the number of words in words.txt. The pre-processing portion (where the word is cleaned of 
apostrophes and accents) takes O(m) time where m is the length of the longest word in the worst-case 
scenario. Since all n words undergo this process, and insertion into the HashMap take O(1) time, the
overarching worst-case time complexity is O(nm) for this program.